{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943effce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dbc43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "import chardet\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "links = [\n",
    "    \"https://www.worldofagri.com/2021/01/chemical-fertilizers-detailed-guide.html?m=1\",\n",
    "    \"https://desertstudiescenter.uoanbar.edu.iq/News_Details.php?ID=192\",\n",
    "    \"https://www.twinkl.com.eg/teaching-wiki/alnbatat-alsamt\",\n",
    "    \"https://safiadecor.com/%D8%AA%D8%B9%D8%B1%D9%81-%D8%B9%D9%84%D9%89-%D8%A7%D9%84%D9%86%D8%A8%D8%A7%D8%AA%D8%A7%D8%AA-%D8%A7%D9%84%D8%B3%D8%A7%D9%85%D8%A9-%D8%A7%D9%84%D8%AF%D8%A7%D8%AE%D9%84%D9%8A%D8%A9-%D9%88-%D8%B7%D8%B1/\",\n",
    "    \"https://hundzsoilegypt.com/%D9%83%D9%85-%D9%8A%D8%AD%D8%AA%D8%A7%D8%AC-%D8%A7%D9%84%D9%81%D8%AF%D8%A7%D9%86-%D9%85%D9%86-%D8%A7%D9%84%D8%B3%D9%85%D8%A7%D8%AF-%D8%A7%D9%84%D8%B9%D8%B6%D9%88%D9%8A/\",\n",
    "    \"https://tajagri.sa/blogs/%D8%A3%D8%B3%D9%85%D8%AF%D8%A9-%D8%B2%D8%B1%D8%A7%D8%B9%D9%8A%D8%A9-%D8%B3%D8%A7%D8%A6%D9%84%D9%87?srsltid=AfmBOoolllLyN--6i6xP33kTziSecL52qWnUTS020wJO_3F__D3H33n6\",\n",
    "    \"https://konyaseker.com.tr/ar/icerik/detay/6293/ar\",\n",
    "    \"https://tajagri.sa/blogs/%D8%B7%D8%B1%D9%8A%D9%82%D8%A9-%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A7%D9%84%D8%B3%D9%85%D8%A7%D8%AF-%D8%A7%D9%84%D8%B3%D8%A7%D8%A6%D9%84-%D9%84%D9%84%D9%86%D8%A8%D8%A7%D8%AA%D8%A7%D8%AA-%D8%A7%D9%84%D8%AF%D8%A7%D8%AE%D9%84%D9%8A%D8%A9?srsltid=AfmBOoo2bTOr8DW1zTSvYhrXrAgc-7NOB4T1G38ig4-BrOewzL50KjVU\"\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read PDF {file_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw = f.read()\n",
    "            encoding = chardet.detect(raw)['encoding']\n",
    "            return raw.decode(encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read TXT file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        return '\\n'.join(para.text for para in doc.paragraphs)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to read DOCX file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for tag in soup(['script', 'style']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=\"\\n\", strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to extract from URL {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_all_text_from_directory(directory_path):\n",
    "    all_text = \"\"\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            print(f\"[PDF] Reading: {filename}\")\n",
    "            all_text += f\"\\n--- PDF: {filename} ---\\n\"\n",
    "            try:\n",
    "                with open(file_path, \"rb\") as f:\n",
    "                    reader = PdfReader(f)\n",
    "                    clean_name = filename.strip().lower()\n",
    "                    start_page = 60 if clean_name == \"الافات والامراض النباتية الجزء الثاني ٢٠٠٣.pdf\" else 0\n",
    "                    for i in range(start_page, len(reader.pages)):\n",
    "                        all_text += reader.pages[i].extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Failed to read PDF {file_path}: {e}\")\n",
    "            all_text += \"\\n\"\n",
    "        elif filename.lower().endswith(\".txt\"):\n",
    "            print(f\"[TXT] Reading: {filename}\")\n",
    "            all_text += f\"\\n--- TXT: {filename} ---\\n\"\n",
    "            all_text += extract_text_from_txt(file_path) + \"\\n\"\n",
    "        elif filename.lower().endswith(\".docx\"):\n",
    "            print(f\"[DOCX] Reading: {filename}\")\n",
    "            all_text += f\"\\n--- DOCX: {filename} ---\\n\"\n",
    "            all_text += extract_text_from_docx(file_path) + \"\\n\"\n",
    "            \n",
    "    for url in links:\n",
    "        print(f\"[URL] Scraping: {url}\")\n",
    "        all_text += f\"\\n--- URL: {url} ---\\n\"\n",
    "        all_text += extract_text_from_url(url) + \"\\n\"\n",
    "\n",
    "    return all_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1abff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TXT] Reading: pests_text.txt\n",
      "[PDF] Reading: الافات والامراض النباتية الجزء الثاني ٢٠٠٣.pdf\n",
      "[DOCX] Reading: النبات الطبي.docx\n",
      "[DOCX] Reading: امراض النبات.docx\n",
      "[URL] Scraping: https://www.worldofagri.com/2021/01/chemical-fertilizers-detailed-guide.html?m=1\n",
      "[URL] Scraping: https://desertstudiescenter.uoanbar.edu.iq/News_Details.php?ID=192\n",
      "[URL] Scraping: https://www.twinkl.com.eg/teaching-wiki/alnbatat-alsamt\n",
      "[URL] Scraping: https://safiadecor.com/%D8%AA%D8%B9%D8%B1%D9%81-%D8%B9%D9%84%D9%89-%D8%A7%D9%84%D9%86%D8%A8%D8%A7%D8%AA%D8%A7%D8%AA-%D8%A7%D9%84%D8%B3%D8%A7%D9%85%D8%A9-%D8%A7%D9%84%D8%AF%D8%A7%D8%AE%D9%84%D9%8A%D8%A9-%D9%88-%D8%B7%D8%B1/\n",
      "[URL] Scraping: https://hundzsoilegypt.com/%D9%83%D9%85-%D9%8A%D8%AD%D8%AA%D8%A7%D8%AC-%D8%A7%D9%84%D9%81%D8%AF%D8%A7%D9%86-%D9%85%D9%86-%D8%A7%D9%84%D8%B3%D9%85%D8%A7%D8%AF-%D8%A7%D9%84%D8%B9%D8%B6%D9%88%D9%8A/\n",
      "[URL] Scraping: https://tajagri.sa/blogs/%D8%A3%D8%B3%D9%85%D8%AF%D8%A9-%D8%B2%D8%B1%D8%A7%D8%B9%D9%8A%D8%A9-%D8%B3%D8%A7%D8%A6%D9%84%D9%87?srsltid=AfmBOoolllLyN--6i6xP33kTziSecL52qWnUTS020wJO_3F__D3H33n6\n",
      "[URL] Scraping: https://konyaseker.com.tr/ar/icerik/detay/6293/ar\n",
      "[URL] Scraping: https://tajagri.sa/blogs/%D8%B7%D8%B1%D9%8A%D9%82%D8%A9-%D8%A7%D8%B3%D8%AA%D8%AE%D8%AF%D8%A7%D9%85-%D8%A7%D9%84%D8%B3%D9%85%D8%A7%D8%AF-%D8%A7%D9%84%D8%B3%D8%A7%D8%A6%D9%84-%D9%84%D9%84%D9%86%D8%A8%D8%A7%D8%AA%D8%A7%D8%AA-%D8%A7%D9%84%D8%AF%D8%A7%D8%AE%D9%84%D9%8A%D8%A9?srsltid=AfmBOoo2bTOr8DW1zTSvYhrXrAgc-7NOB4T1G38ig4-BrOewzL50KjVU\n",
      "✅ All text extracted and saved to 'extracted_output.txt'\n"
     ]
    }
   ],
   "source": [
    "extracted_data = extract_all_text_from_directory(directory_path='Data/')\n",
    "with open(\"extracted_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(extracted_data)\n",
    "print(\"✅ All text extracted and saved to 'extracted_output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cfd7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    text_chunks = text_splitter.split_text(extracted_data)  \n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a68a1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text chunks : 1511\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of text chunks :\",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2538d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "env_path = Path(\"D:/Artificial intelligence/nlp/plants_chatpot/.env\")\n",
    "load_dotenv(env_path)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\", max_retries: int = 3):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                text = text.replace(\"\\n\", \" \")\n",
    "                response = openai.embeddings.create(\n",
    "                    input=[text],\n",
    "                    model=model\n",
    "                )\n",
    "                return response.data[0].embedding\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Final error getting embedding: {e}\")\n",
    "                    return None\n",
    "                print(f\"Error getting embedding (attempt {attempt + 1}): {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "INDEX_DIR = r'D:\\Artificial intelligence\\nlp\\plants_chatpot\\index'\n",
    "index_path = Path(INDEX_DIR) / 'index.faiss'\n",
    "chunks_path = Path(INDEX_DIR) / 'chunks.json'\n",
    "\n",
    "def build_index(chunks: List[str]):\n",
    "        embeddings_list = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            embedding = get_embedding(chunk)\n",
    "            if embedding:\n",
    "                embeddings_list.append(embedding)\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        if not embeddings_list:\n",
    "            raise ValueError(\"No valid embeddings were generated\")\n",
    "            \n",
    "        embeddings_array = np.array(embeddings_list).astype('float32')\n",
    "        dimension = len(embeddings_list[0])\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings_array)\n",
    "        faiss.write_index(index, str(index_path))\n",
    "        with open(chunks_path, 'w') as f:\n",
    "            json.dump(chunks, f)\n",
    "        \n",
    "        print(f\"Index built successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d905f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built successfully\n"
     ]
    }
   ],
   "source": [
    "build_index(chunks=text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0b57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
